  
This project will aim to integrate computer vision and audio synthesis to explore the cross-modal translation of musical performances, where visual information from videos of musicians could be used to synthesize accurate and emotive audio. We aim to develop a model capable of generating music by combining visual performance data, such as body language, facial expressions, and instrumental techniques, with musical scores. The system will leverage cross-modal embedding techniques to translate features from the visual domain into the auditory domain, potentially allowing for enriched understanding of the relationship between the visual performance of music and the underlying musical language and further investigation into performance analysis. A key challenge will be determining how to extract and utilize features from performance videos, such as the motion of a violin bow, fingering techniques, and features unrelated to the music itself, such as facial expression, clothing, setting, and body language, to influence the resulting synthesized audio.

A key part of this project’s look into translating across multimodal feature spaces is how these cross-modality features can be aligned contextually using transformer networks with positional encodings or another architectural framework (e.g., capsule networks). This project will explore how changes in the visual modality, such as technique, emotion, or potentially clothing, could influence the synthesized music’s properties. To evaluate the accuracy of music synthesis, there are two approaches: Through objective measures (i.e., how accurately can we recreate the original audio and does the model correctly identify themes such as genre, emotion, or skill level). Alternatively, evaluation could be completed via subjective assessments, where human listeners rate the authenticity and emotional impact of the generated music, and this can be compared to the model’s “understanding” of the music it is generating. The exploration of genre detection and musical sentiment analysis based on performance videos will be important, potentially leading to a way of synthesizing music that can reflect the features of music only conveyed by the visual component of a performance.

The project will explore various implementation approaches, potentially using supervised learning with paired audio-(video+score) data or generative adversarial networks. Additionally, this research could consider employing reinforcement learning techniques to fine-tune the model’s ability to replicate music and the nuances not captured within a simple score. This work could extend to traverse these modalities in the other direction. By potentially reversing the network and generating a synthetic performance video (based on some prior) from a piece of music that can include the features learned by the model. I.e. the model understands the relationship between a musical theme and a performer’s facial expression.
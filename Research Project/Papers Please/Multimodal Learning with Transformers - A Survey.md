[Link]()

Multimodal Learning (MML) e.g. audio+visual speech recognition applications.

Transformers are well suited to different modalities, as tokenization helps them process different modalities universally.

ViTs - image specific input pipeline that splits the image up into patches.

>[!quote]  Multimodal Input - *The Transformer family is a general architecture that can be formulated as a type of general graph neural network. Specifically, self-attention can process each input as a fully-connected graph, by attending to the global (non-local) patterns. Therefore, this intrinsic trait helps Transformers can work in a modality agnostic pipeline that is compatible with various modalities by treating the embedding of each token as a node of the graph.*

>[!quote] Multimodal Embeddings - *Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.*
